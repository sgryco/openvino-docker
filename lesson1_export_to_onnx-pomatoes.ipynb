{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a network for production to Onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is base on the lesson 1, Image classification with Convolutional Neural Networks.\n",
    "Here, we will see how to train our own image classifier to differentiate tomatoes from potatoes.\n",
    "We will then export the network to the Onnx format.\n",
    "Then we will use OpenVino to convert the model and run the inference on the neural compute stick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *\n",
    "\n",
    "# Those imports are used for accessing the underlying Pytorch model\n",
    "from torch.nn import Softmax, Sequential\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"potatoestomatoes/\"\n",
    "sz=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch=resnet34\n",
    "data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))\n",
    "learn = ConvLearner.pretrained(arch, data, precompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5481ed492a7e485793cc5cdbfc3dcb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3, style=ProgressStyle(description_width='initialâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      0.377315   0.055485   0.984925  \n",
      "    1      0.220223   0.019195   0.994975        \n",
      "    2      0.146355   0.01567    0.994975        \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015670226896228504, 0.9949748743718593]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(0.01, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to acess preprocessing parameters\n",
    "tfms=tfms_from_model(arch, sz)\n",
    "t = tfms[0]\n",
    "dir(t)\n",
    "m,s = t.norm.m, t.norm.s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199, 2)\n",
      "[[ -0.00141  -6.56288]\n",
      " [ -0.00001 -11.96286]\n",
      " [ -0.00002 -10.8046 ]]\n",
      "[[-10.62268  -0.00002]\n",
      " [ -7.77634  -0.00042]\n",
      " [-10.00165  -0.00005]]\n",
      "Accuracy 99.50%\n"
     ]
    }
   ],
   "source": [
    "log_preds = learn.predict()\n",
    "print(log_preds.shape)\n",
    "print(log_preds[:3])\n",
    "print(log_preds[-3:])\n",
    "acc = (data.val_y == np.argmax(log_preds, axis=1)).mean() * 100.\n",
    "print('Accuracy {:0.2f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting to Onnx\n",
    "As OpenVino Onnx plugin does not support the LogSoftMax layer (See [here](https://software.intel.com/en-us/articles/OpenVINO-Using-ONNX#supported-onnx-layers)), we replace the last layer of the model from a LogSoftMax to a SoftMax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Export done'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "# disable precompute to enable the full resnet model + the new fc layers \n",
    "learn.precompute = False\n",
    "# disable training mode, (disabling barch normalisation, dropout, etc)\n",
    "learn.model.train(False)\n",
    "\n",
    "dummy_input = Variable(torch.randn(1, 3, sz, sz), requires_grad=True)\n",
    "model2 = Sequential(copy.deepcopy(learn.model[:-1]), Softmax())\n",
    "model2 = model2.to('cpu')\n",
    "model2.train(False)\n",
    "torch.onnx.export(model2, dummy_input, 'Modelpotatoestomatoes_softmax.onnx')\n",
    "\"Export done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When going to production, we also need to export the preprocessing done to the images,\n",
    "here the images are converted to 0-1, resized have the min dimension to be 224, cropped centered,\n",
    "then we substract the mean and divide by the scale to normilize them.\n",
    "Finally, we reshape them to be Channel, Width, Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.485, 0.456, 0.406], dtype=float32),\n",
       " array([0.229, 0.224, 0.225], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save mean and scale to use in your inference program\n",
    "trn_tfms, val_tfms = tfms_from_model(arch,sz)\n",
    "mean, scale = val_tfms.norm.m, val_tfms.norm.s \n",
    "mean, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['potatoes', 'tomatoes']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we need to save the class names, as the network will only output the class indicies\n",
    "data.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test image preprocessing\n",
    "Here we run some test to validate that we have the current preprocessing function by comparing our image loading function with the one from fastai/pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 224, 224)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first 5 validation images\n",
    "valImgs = data.val_ds[:5][0]\n",
    "valImgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn.predict Accuracy 100.00%\n",
      "softmax Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "preds = learn.predict_array(valImgs)\n",
    "acc = (data.val_y[:5] == np.argmax(preds, axis=1)).mean() * 100.\n",
    "print('learn.predict Accuracy {:0.2f}%'.format(acc))\n",
    "\n",
    "#second model with SoftMax instead of LogSoftMax\n",
    "out = model2(torch.from_numpy(valImgs))\n",
    "softmax = out.detach().numpy()\n",
    "\n",
    "acc = (data.val_y[:5] == np.argmax(softmax, axis=1)).mean() * 100.\n",
    "print('softmax Accuracy {:0.2f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['potatoes', 'tomatoes']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here, we compare the result of the inference when using our own preprocessing (imt) with the fastai preprocessing for the original model and the model with the SoftMaxlayer:\n",
      "Processing image potatoestomatoes//valid/potatoes/10. 1557cbd06293972.jpg\n",
      "predict tfms [[-0.00305 -5.79355]]\n",
      "predict imt [[-0.00305 -5.79355]]\n",
      "model2  imt tensor([[0.9970, 0.0030]], grad_fn=<SoftmaxBackward>)\n",
      "Processing image potatoestomatoes/valid/tomatoes/10. fried-green-tomatoes1.jpg\n",
      "predict tfms [[-6.71964 -0.00121]]\n",
      "predict imt [[-6.71964 -0.00121]]\n",
      "model2  imt tensor([[0.0012, 0.9988]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "impaths = [f\"{PATH}/valid/potatoes/10. 1557cbd06293972.jpg\", f\"{PATH}valid/tomatoes/10. fried-green-tomatoes1.jpg\"]\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "learn.precompute = False\n",
    "print('Here, we compare the result of the inference when using our own preprocessing (imt) '\n",
    "      'with the fastai preprocessing for the original model and the model with the SoftMaxlayer:')\n",
    "for imp in impaths:\n",
    "    print(f\"Processing image {imp}\")\n",
    "    imfast = open_image(imp)\n",
    "    # open image to [0-1] RGB, Width Height Channel\n",
    "    imraw = cv2.imread(imp).astype(np.float32)/255\n",
    "    imraw = cv2.cvtColor(imraw, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # resize and crop center\n",
    "    r,c, *_ = imraw.shape\n",
    "    ratio = sz/min(r, c)\n",
    "    im = cv2.resize(imraw, (max(math.floor(c * ratio), sz), max(math.floor(r * ratio), sz)), interpolation=cv2.INTER_AREA)\n",
    "    startx = int(np.ceil((im.shape[0] - sz) / 2 ))\n",
    "    starty = int(np.ceil((im.shape[1] - sz) / 2 ))\n",
    "    im = im[startx:startx + sz, starty:starty + sz]\n",
    "    # normalize\n",
    "    imt = (im - mean) / scale\n",
    "    imt = imt.transpose((2, 0, 1))\n",
    "    print(\"predict tfms\", learn.predict_array(val_tfms(imraw)[None]))\n",
    "    print(\"predict imt\", learn.predict_array(imt[None]))\n",
    "    print(\"model2  imt\", model2(torch.from_numpy(imt[None])))\n",
    "\n",
    "    \n",
    "    #print(\"imfast\", learn.predict_array(val_tfms(imfast)[None]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
